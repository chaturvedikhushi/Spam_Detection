# -*- coding: utf-8 -*-
"""Spam-detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aL2MHJOJ8S0_mNNdKcy_ucU7CgAhtE9A
"""

import pandas as pd

# Specify the encoding parameter
Data = pd.read_csv('spam.csv', encoding='ISO-8859-1')
Data.head()

Data.shape

# Data cleaning
# EDA
# Text preprocessing
# Model building
# Evaluation
# Improvement
# Website
# Deploy

"""1. DATA CLEANING

"""

Data.info

# drop last 3 column
Data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)

Data.sample(5)

#rename columns
Data.rename(columns={'v1':'target','v2':'text'},inplace=True)
Data.sample(5)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

Data['target'] = encoder.fit_transform(Data['target'])

Data.sample(5)

#missing value
Data.isnull().sum()

#check for duplicate value
Data.duplicated().sum()

#remove duplicate
Data=Data.drop_duplicates(keep='first')

Data.duplicated().sum()

Data.shape

"""2. EDA"""

Data.head()

Data['target'].value_counts()

from matplotlib import pyplot as plt
plt.pie(Data['target'].value_counts(),  labels=['ham','spam'],autopct="%.2f")
#data is imbalanced

#Natural language toolkit
import nltk

!pip install nltk

#Dependices
nltk.download('punkt')

#Number of characters
Data['num_characters'] = Data['text'].apply(len)

Data.head()

#Number of words
Data['num_words'] = Data['text'].apply(lambda x:len(nltk.word_tokenize(x)))

Data.head()

Data['num_sentences'] = Data['text'].apply(lambda x:len(nltk.sent_tokenize(x)))
Data['num_sentences']

Data.head()

Data[['num_characters', 'num_words','num_sentences']].describe()

#For ham messages
Data[Data['target'] == 0][['num_characters', 'num_words','num_sentences']].describe()

#For spam messages
Data[Data['target'] == 1][['num_characters', 'num_words','num_sentences']].describe()

import seaborn as sns

plt.figure(figsize=(12,6))
sns.histplot(Data[Data['target'] == 0]['num_characters'])
sns.histplot(Data[Data['target'] == 1]['num_characters'], color='red')

plt.figure(figsize=(12,6))
sns.histplot(Data[Data['target'] == 0]['num_words'])
sns.histplot(Data[Data['target'] == 1]['num_words'], color='red')

sns.pairplot(Data, hue='target')

sns.heatmap(Data.corr(),annot=True)

"""3 . Data Preprocessing
* Lower case
* Tokenization
* Removing special characters
* Removing stop words and punctuation
* Stemming #similar types words convert into one form example
dancing, dancer , dance
"""

import nltk
from nltk.corpus import stopwords
import string

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
      y.append(ps.stem(i))

    return " ".join(y)

transform_text("HELLO my name 20% are you  coming this dancing loving")

Data['transformed_text'] = Data['text'].apply(transform_text)
Data['transformed_text']

Data.head()

#wordcloud of spam message
from wordcloud import WordCloud
wc = WordCloud(width = 500 , height=500,min_font_size=10, background_color='white')

spam_wc = wc.generate(Data[Data['target'] == 1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

ham_wc = wc.generate(Data[Data['target'] == 0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(ham_wc)

#Top 30 or 50 words in spam message
spam_corpus=[]
for msg in Data[Data['target'] == 1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

#Most use 30 words in spam messages
# Assuming spam_corpus is a list of words in your spam corpus
from collections import Counter
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a Counter for word occurrences
word_counter = Counter(spam_corpus)

# Create a DataFrame from the Counter
df_word_counts = pd.DataFrame(word_counter.most_common(30), columns=['Word', 'Count'])

# Plot the bar plot using Seaborn
sns.barplot(x='Word', y='Count', data=df_word_counts)
plt.xticks(rotation='vertical')
plt.show()

#Top 30 or 50 words in ham message
ham_corpus=[]
for msg in Data[Data['target'] == 0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

len(ham_corpus)

#Most use 30 words in spam messages
# Assuming spam_corpus is a list of words in your spam corpus
from collections import Counter
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a Counter for word occurrences
word_counter = Counter(ham_corpus)

# Create a DataFrame from the Counter
df_word_counts = pd.DataFrame(word_counter.most_common(30), columns=['Word', 'Count'])

# Plot the bar plot using Seaborn
sns.barplot(x='Word', y='Count', data=df_word_counts)
plt.xticks(rotation='vertical')
plt.show()

"""4. Model Building  
Naive Bayes For textual data
"""

#Convert text data into numberic matrix
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features=3000)

X = tfidf.fit_transform(Data['transformed_text']).toarray()

X.shape #sms, words

y = Data['target'].values
y

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB

gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(X_train, y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_pred1, y_test))
print(confusion_matrix(y_pred1, y_test))
print(precision_score(y_pred1, y_test))

#Select this one
mnb.fit(X_train, y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_pred2, y_test))
print(confusion_matrix(y_pred2, y_test))
print(precision_score(y_pred2, y_test))

bnb.fit(X_train, y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_pred3, y_test))
print(confusion_matrix(y_pred3, y_test))
print(precision_score(y_pred3, y_test))

#tdidf - mnb

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb = XGBClassifier(n_estimators=50,random_state=2)

clfs = {
    'SVC' : svc,
    'KN' : knc,
    'NB': mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'ETC': etc,
    'GBDT':gbdt,
    'xgb':xgb
}

def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)

    return accuracy,precision

train_classifier(svc,X_train,y_train,X_test,y_test)

accuracy_scores = []
precision_scores = []

for name,clf in clfs.items():

    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

performance_df

performance_df1 = pd.melt(performance_df, id_vars = "Algorithm")

performance_df1

sns.catplot(x = 'Algorithm', y='value',
               hue = 'variable',data=performance_df1, kind='bar',height=5)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()

# model improve
# 1. Change the max_features parameter of TfIdf

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_scaling':accuracy_scores,'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)

new_df = performance_df.merge(temp_df,on='Algorithm')

new_df_scaled = new_df.merge(temp_df,on='Algorithm')

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_num_chars':accuracy_scores,'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)

new_df_scaled.merge(temp_df,on='Algorithm')

# Voting Classifier
svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)
mnb = MultinomialNB()
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')

voting.fit(X_train,y_train)

y_pred = voting.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

# Applying stacking
estimators=[('svm', svc), ('nb', mnb), ('et', etc)]
final_estimator=RandomForestClassifier()

from sklearn.ensemble import StackingClassifier

clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))